### 二分类问题

* 符号定义：
> $h_{\theta}(x)$ 表示$x$样本为正样本的概率
> 
> $y$表示该样本实际为正还是负样本，二分类中只有0,1
> 
> $m$为训练集中的样本个数
>
>$x^{(i)}$表示第i个样本, $x^{(i)}_{j}$表示第i个样本的第j个分量



* 单个样本的损失为(交叉熵代价函数)：

$$

    Cost(h_{\theta}(x),y)=-[y*log(h_{\theta}(x))+(1-y)log(1-h_{\theta}(x)])

$$

* 对于训练集全体样本损失为:

$$

    J(\theta)=-\frac{1}{m}\sum^{m}_{i=1}[y^{(i)}*log(h_{\theta}(x^{(i)}))+(1-y^{(i)})log(1-h_{\theta}(x^{(i)})])

$$

* 利用极大似然估计推导交叉熵

$$

    p(y=1|x,\theta)=h_{\theta}(x)
    \\
    p(y=0|x,\theta)=1-h_{\theta}(x)

$$

* 合并上述两个式子有:

$$

    p(y|x,\theta)=(h_{\theta}(x)^{y})(1-h_{\theta}(x))^{1-y}

$$

* 对m个样本，求极大似然估计，得到

$$
\begin{aligned}

    & L(\theta)=\prod_{i=1}^{m}p(y^{(i)}|x^{(i)};\theta)\\
    &=\prod_{i=1}^{m}(h_{\theta}(x)^{y})(1-h_{\theta}(x))^{1-y}
    
\end{aligned}
$$

* 取对数似然得到:

$$
\begin{aligned}

    & l(\theta)=logL(\theta)\\
    & = \sum_{i=1}^{m}y^{(i)}*log(h_{\theta}(x^{(i)}))+(1-y^{(i)})log(1-h_{\theta}(x^{(i)})

\end{aligned}
$$

当为逻辑回归的的时候,$h_{\theta}(x)=\frac{1}{1+e^{-x^{T}\theta}}$ ,最大化对数似然也即极小化$-l(\theta)$

* 优化方案，梯度下降法，也即对负对数似然求偏导数

$$

    \frac{\partial{J(\theta)}}{\partial(\theta_{j})}=\frac{1}{m}\sum_{i=1}^{m}(h_{\theta}(x^{(i)})-y^{(i)})x^{(i)}_{j}

$$

### 多分类问题

* 符号说明

> k 是类别个数

> m 是样本数量

> $1\{.\}$表示示性函数,当括号中为真值，函数值为1，否则为0

* 建立目标损失函数(对数似然代价函数)
  
$$

    J(w,b)=-\frac{1}{m}\sum_{j=1}^{m}\sum_{l=1}^{k}1\{y^{(j)}=l\}log(\frac{e^{w_{l}x^{(j)}+b_{l}}}{\sum_{i=1}^{k}e^{w_{i}x^{(j)}+b_{i}}})

$$

* 由对数的性质展开等式得到

$$

    J(w,b)=-\frac{1}{m}\sum_{j=1}^{m}\sum_{l=1}^{k}
    (1\{y^{(j)}=l\}log(e^{w_{l}x^{(j)}+b_{l}})-1\{y^{(j)}=l\}log(\sum_{i=1}^{k}e^{w_{i}x^{(j)}+b_{i}}))

$$

* 继续展开得到

$$

    J(w,b)=-\frac{1}{m}\sum_{j=1}^{m}(\sum_{l=1}^{k}
    1\{y^{(j)}=l\}log(e^{w_{l}x^{(j)}+b_{l}})-log(\sum_{i=1}^{k}e^{w_{i}x^{(j)}+b_{i}}))

$$

* 对$w_{i}$求偏导得,即对一个向量求导

$$

    \frac{\partial{J(\theta)}}{\partial{w_{r}}}=-\frac{1}{m}\sum_{j=1}^{m}(\sum_{l=1}^{k}1\{y^{(j)}=l\}\frac{e^{w_{l}x^{(j)}+b_{l}}*x^{(j)}}{e^{w_{l}x^{(j)}+b_{l}}}-\frac{e^{w_{r}x^{(j)}+b_{i}}*x^{(j)}}{\sum_{i=1}^{k}e^{w_{i}*x^{(j)}+b_{i}}})

$$

* 化简可得

$$

    \frac{\partial{J(\theta)}}{\partial{w_{r}}}=-\frac{1}{m}\sum_{j=1}^{m}(\sum_{l=1}^{k}1\{y^{(j)}=l\}x^{(j)}-P(y^{(i)}=r|x^{(j)})*x^{(j)})

$$

* 最终化简得到

$$

    \frac{\partial{J(\theta)}}{\partial{w_{r}}}=-\frac{1}{m}\sum_{j=1}^{m}(x^{(j)}-P(y^{(i)}=r|x^{(j)})*x^{(j)})

$$